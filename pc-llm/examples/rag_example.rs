//! Example: RAG (Retrieval Augmented Generation)

use pc_llm::{PersonalControllerLlm, LlmConfig};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    tracing_subscriber::fmt::init();

    println!("üîç Personal Controller LLM - RAG Example\n");

    // Create LLM with RAG enabled
    let config = LlmConfig {
        model: "local".to_string(),
        use_rag: true,
        rag_top_k: 5,
        ..Default::default()
    };

    let mut llm = PersonalControllerLlm::new(config)?;
    llm.initialize().await?;

    println!("‚úì LLM with RAG initialized\n");

    // Complex queries that benefit from RAG
    let queries = vec![
        "Quais s√£o as principais rotas de transporte da √Åvila?",
        "Qual o valor m√©dio dos fretes para S√£o Paulo?",
        "Quantos funcion√°rios trabalharam mais de 40 horas esta semana?",
    ];

    for query in queries {
        println!("üë§ Query: {}", query);

        let response = llm.chat(query).await?;

        println!("ü§ñ Response: {}", response.response);

        if !response.sources.is_empty() {
            println!("üìö Sources retrieved:");
            for (i, source) in response.sources.iter().enumerate() {
                println!("   {}. {}", i + 1, source);
            }
        } else {
            println!("‚ÑπÔ∏è  No sources retrieved (RAG not yet connected to AvilaDB)");
        }

        println!();
    }

    Ok(())
}
